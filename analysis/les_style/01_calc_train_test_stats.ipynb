{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a notebook to load all the training data or load all the testing data, and then we calculate different quantities. We incrementally open up and analyze each dataset because these are large datasets, and loading them takes quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load files\n",
    "dir_in_train = Path('/scratch/orybchuk/wakedynamics/ldm-3d/simulations/train/nc_trimmed')\n",
    "train_files = list(Path(dir_in_train).glob('*.nc'))\n",
    "train_files.sort()\n",
    "\n",
    "ds_train = xr.open_mfdataset(train_files, parallel=True, engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vertical profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_train_mean = ds_train['u'].mean(('x', 'y', 'time')).compute()\n",
    "# u_train_std = ds_train['u'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_train_mean = ds_train['v'].mean(('x', 'y', 'time')).compute()\n",
    "# v_train_std = ds_train['v'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_train_mean = ds_train['w'].mean(('x', 'y', 'time')).compute()\n",
    "# w_train_std = ds_train['w'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_train_mean.to_netcdf('u_train_mean.nc')\n",
    "# u_train_std.to_netcdf('u_train_std.nc')\n",
    "# v_train_mean.to_netcdf('v_train_mean.nc')\n",
    "# v_train_std.to_netcdf('v_train_std.nc')\n",
    "# w_train_mean.to_netcdf('w_train_mean.nc')\n",
    "# w_train_std.to_netcdf('w_train_std.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train[\"up\"] = ds_train['u'] - ds_train['u'].mean(('x', 'y'))\n",
    "# ds_train[\"vp\"] = ds_train['v'] - ds_train['v'].mean(('x', 'y'))\n",
    "# ds_train[\"wp\"] = ds_train['w'] - ds_train['w'].mean(('x', 'y'))\n",
    "\n",
    "# ds_train[\"upvp\"] = ds_train['up'] * ds_train['vp']\n",
    "# ds_train[\"upwp\"] = ds_train['up'] * ds_train['wp']\n",
    "# ds_train[\"vpwp\"] = ds_train['vp'] * ds_train['wp']\n",
    "\n",
    "# ds_train[\"upvp\"].compute()\n",
    "# ds_train[\"upwp\"].compute()\n",
    "# ds_train[\"vpwp\"].compute()\n",
    "\n",
    "# ds_train['upvp'].to_netcdf(\"upvp_train.nc\")\n",
    "# ds_train['upwp'].to_netcdf(\"upwp_train.nc\")\n",
    "# ds_train['vpwp'].to_netcdf(\"vpwp_train.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upvp_train_mean = ds_train['upvp'].mean(('x', 'y', 'time')).compute()\n",
    "# upwp_train_mean = ds_train['upwp'].mean(('x', 'y', 'time')).compute()\n",
    "# vpwp_train_mean = ds_train['vpwp'].mean(('x', 'y', 'time')).compute()\n",
    "\n",
    "# upvp_train_mean.to_netcdf(\"upvp_train_profile.nc\")\n",
    "# upwp_train_mean.to_netcdf(\"upwp_train_profile.nc\")\n",
    "# vpwp_train_mean.to_netcdf(\"vpwp_train_profile.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate histogram, going sample by sample\n",
    "## Bins\n",
    "umin = 6\n",
    "umax = 12\n",
    "ubins = np.linspace(umin, umax, 500)\n",
    "\n",
    "vmin = -1.5\n",
    "vmax = 3.5\n",
    "vbins = np.linspace(vmin, vmax, 500)\n",
    "\n",
    "wmin = -0.75\n",
    "wmax = 0.75\n",
    "wbins = np.linspace(wmin, wmax, 300)\n",
    "\n",
    "## Scipy histograms\n",
    "uhist_np = np.histogram(ds_train['u'].values.flatten(), bins=ubins)\n",
    "uhist = scipy.stats.rv_histogram(uhist_np)\n",
    "\n",
    "vhist_np = np.histogram(ds_train['v'].values.flatten(), bins=vbins)\n",
    "vhist = scipy.stats.rv_histogram(vhist_np)\n",
    "\n",
    "whist_np = np.histogram(ds_train['w'].values.flatten(), bins=wbins)\n",
    "whist = scipy.stats.rv_histogram(whist_np)\n",
    "\n",
    "np.save('u_train_hist.npy', uhist.pdf(ubins))\n",
    "np.save('v_train_hist.npy', vhist.pdf(vbins))\n",
    "np.save('w_train_hist.npy', whist.pdf(wbins))\n",
    "\n",
    "# ## Calculate histograms - OLD WAY\n",
    "# all_hist_u = np.zeros((len(ds_train['time']), len(ubins)))\n",
    "# all_hist_v = np.zeros((len(ds_train['time']), len(vbins)))\n",
    "# all_hist_w = np.zeros((len(ds_train['time']), len(wbins)))\n",
    "# for i in range(len(ds_train['time'])):\n",
    "#     uhist_train_np = np.histogram(ds_train['u'].isel(time=i).values.flatten(), bins=ubins)\n",
    "#     uhist_train = scipy.stats.rv_histogram(uhist_train_np)\n",
    "#     all_hist_u[i,:] = uhist_train.pdf(ubins).copy()\n",
    "    \n",
    "#     vhist_train_np = np.histogram(ds_train['v'].isel(time=i).values.flatten(), bins=vbins)\n",
    "#     vhist_train = scipy.stats.rv_histogram(vhist_train_np)\n",
    "#     all_hist_v[i,:] = vhist_train.pdf(vbins).copy()\n",
    "    \n",
    "#     whist_train_np = np.histogram(ds_train['w'].isel(time=i).values.flatten(), bins=wbins)\n",
    "#     whist_train = scipy.stats.rv_histogram(whist_train_np)\n",
    "#     all_hist_w[i,:] = whist_train.pdf(wbins).copy()\n",
    "    \n",
    "# np.save('u_train_hist.npy', all_hist_u)\n",
    "# np.save('v_train_hist.npy', all_hist_v)\n",
    "# np.save('w_train_hist.npy', all_hist_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate 1D spectra, sample by sample\n",
    "# khub = 6\n",
    "# tmp1 = ds_train['up'].isel(time=0, z=khub)\n",
    "# tmp2 = np.fft.rfftn(tmp1.values, axes=(0,))\n",
    "# all_spectra = np.zeros((len(ds_train['time']), len(tmp2)))    \n",
    "    \n",
    "# for i in range(len(ds_train['time'])):\n",
    "#     up_xy_train = ds_train['up'].isel(time=i, z=khub)\n",
    "#     up_k_train = np.fft.rfftn(up_xy_train.values, axes=(0,))\n",
    "#     up_k_bar_train = np.mean(np.abs(up_k_train)**2, axis=1)\n",
    "    \n",
    "#     all_spectra[i,:] = up_k_bar_train.copy()\n",
    "# np.save('spectra_train.npy', all_spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vel = xr.open_dataset(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate histograms that assess continuity\n",
    "# ## Helper variables\n",
    "# # Simulation parameters\n",
    "# dx, dy, dz = 15, 15, 15\n",
    "# facx, facy, facz = 1/(4*dx), 1/(4*dy), 1/(4*dz)\n",
    "\n",
    "# # Histogram parameters\n",
    "# dmin = -0.02\n",
    "# dmax = 0.02\n",
    "# dbins = np.linspace(dmin, dmax, 500)\n",
    "# all_dudx_hist = np.zeros((len(ds_train['time']), len(dbins)))\n",
    "# all_dvdy_hist = np.zeros((len(ds_train['time']), len(dbins)))\n",
    "# all_dwdz_hist = np.zeros((len(ds_train['time']), len(dbins)))\n",
    "# all_div_hist = np.zeros((len(ds_train['time']), len(dbins)))\n",
    "\n",
    "# ## Iterate over timesteps\n",
    "# for t in range(len(ds_train['time'])):\n",
    "#     if t % 100 == 0: print(t, '...')\n",
    "# #     vel = ds_train.isel(time=t)\n",
    "#     vel = xr.open_dataset(train_files[t])\n",
    "\n",
    "#     # Calculate gradients and divergence\n",
    "#     dudx = facx*(-vel['u'].roll(x=-1,y=-1,z=-1,roll_coords=False).values + vel['u'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "#                  -vel['u'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values + vel['u'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "#                  -vel['u'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values + vel['u'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "#                  -vel['u'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['u'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "#     dvdy = facy*(-vel['v'].roll(x=-1,y=-1,z=-1,roll_coords=False).values - vel['v'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "#                  +vel['v'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values + vel['v'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "#                  -vel['v'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values - vel['v'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "#                  +vel['v'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['v'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "#     dwdz = facz*(-vel['w'].roll(x=-1,y=-1,z=-1,roll_coords=False).values - vel['w'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "#                  -vel['w'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values - vel['w'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "#                  +vel['w'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values + vel['w'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "#                  +vel['w'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['w'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "#     div = dudx + dvdy + dwdz\n",
    "\n",
    "#     # Calculate histograms\n",
    "#     dudxhist_np = np.histogram(dudx.flatten(), bins=dbins)\n",
    "#     dudxhist = scipy.stats.rv_histogram(dudxhist_np)\n",
    "\n",
    "#     dvdyhist_np = np.histogram(dvdy.flatten(), bins=dbins)\n",
    "#     dvdyhist = scipy.stats.rv_histogram(dvdyhist_np)\n",
    "\n",
    "#     dwdzhist_np = np.histogram(dwdz.flatten(), bins=dbins)\n",
    "#     dwdzhist = scipy.stats.rv_histogram(dwdzhist_np)\n",
    "\n",
    "#     divhist_np = np.histogram(div.flatten(), bins=dbins)\n",
    "#     divhist = scipy.stats.rv_histogram(divhist_np)\n",
    "\n",
    "#     # Store histograms\n",
    "#     all_dudx_hist[t,:] = dudxhist.pdf(dbins)\n",
    "#     all_dvdy_hist[t,:] = dvdyhist.pdf(dbins)\n",
    "#     all_dwdz_hist[t,:] = dwdzhist.pdf(dbins)\n",
    "#     all_div_hist[t,:] = divhist.pdf(dbins)\n",
    "\n",
    "# # Save histograms\n",
    "# np.save('hist_dudx_train.npy', all_dudx_hist.mean(axis=0))\n",
    "# np.save('hist_dvdy_train.npy', all_dvdy_hist.mean(axis=0))\n",
    "# np.save('hist_dwdz_train.npy', all_dwdz_hist.mean(axis=0))\n",
    "# np.save('hist_div_train.npy', all_div_hist.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all testing data (81 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load files\n",
    "dir_in_test = Path('/scratch/orybchuk/wakedynamics/ldm-3d/simulations/test/nc_trimmed')\n",
    "test_files = list(Path(dir_in_test).glob('*.nc'))\n",
    "test_files.sort()\n",
    "\n",
    "ds_test = xr.open_mfdataset(test_files, parallel=True, engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vertical profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_test_mean = ds_test['u'].mean(('x', 'y', 'time')).compute()\n",
    "# u_test_std = ds_test['u'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_test_mean = ds_test['v'].mean(('x', 'y', 'time')).compute()\n",
    "# v_test_std = ds_test['v'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_test_mean = ds_test['w'].mean(('x', 'y', 'time')).compute()\n",
    "# w_test_std = ds_test['w'].std(('x', 'y', 'time')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_test_mean.to_netcdf('u_test_mean.nc')\n",
    "# u_test_std.to_netcdf('u_test_std.nc')\n",
    "# v_test_mean.to_netcdf('v_test_mean.nc')\n",
    "# v_test_std.to_netcdf('v_test_std.nc')\n",
    "# w_test_mean.to_netcdf('w_test_mean.nc')\n",
    "# w_test_std.to_netcdf('w_test_std.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test[\"up\"] = ds_test['u'] - ds_test['u'].mean(('x', 'y'))\n",
    "# ds_test[\"vp\"] = ds_test['v'] - ds_test['v'].mean(('x', 'y'))\n",
    "# ds_test[\"wp\"] = ds_test['w'] - ds_test['w'].mean(('x', 'y'))\n",
    "\n",
    "# ds_test[\"upvp\"] = ds_test['up'] * ds_test['vp']\n",
    "# ds_test[\"upwp\"] = ds_test['up'] * ds_test['wp']\n",
    "# ds_test[\"vpwp\"] = ds_test['vp'] * ds_test['wp']\n",
    "\n",
    "# ds_test[\"upvp\"].compute()\n",
    "# ds_test[\"upwp\"].compute()\n",
    "# ds_test[\"vpwp\"].compute()\n",
    "\n",
    "# ds_test['upvp'].to_netcdf(\"upvp_test.nc\")\n",
    "# ds_test['upwp'].to_netcdf(\"upwp_test.nc\")\n",
    "# ds_test['vpwp'].to_netcdf(\"vpwp_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upvp_test_mean = ds_test['upvp'].mean(('x', 'y', 'time')).compute()\n",
    "# upwp_test_mean = ds_test['upwp'].mean(('x', 'y', 'time')).compute()\n",
    "# vpwp_test_mean = ds_test['vpwp'].mean(('x', 'y', 'time')).compute()\n",
    "\n",
    "# upvp_test_mean.to_netcdf(\"upvp_test_profile.nc\")\n",
    "# upwp_test_mean.to_netcdf(\"upwp_test_profile.nc\")\n",
    "# vpwp_test_mean.to_netcdf(\"vpwp_test_profile.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate histogram, going sample by sample\n",
    "# ## Bins\n",
    "# umin = 6\n",
    "# umax = 12\n",
    "# ubins = np.linspace(umin, umax, 500)\n",
    "\n",
    "# vmin = -1.5\n",
    "# vmax = 3.5\n",
    "# vbins = np.linspace(vmin, vmax, 500)\n",
    "\n",
    "# wmin = -0.75\n",
    "# wmax = 0.75\n",
    "# wbins = np.linspace(wmin, wmax, 300)\n",
    "\n",
    "# ## Scipy histograms\n",
    "# uhist_np = np.histogram(ds_test['u'].values.flatten(), bins=ubins)\n",
    "# uhist = scipy.stats.rv_histogram(uhist_np)\n",
    "\n",
    "# vhist_np = np.histogram(ds_test['v'].values.flatten(), bins=vbins)\n",
    "# vhist = scipy.stats.rv_histogram(vhist_np)\n",
    "\n",
    "# whist_np = np.histogram(ds_test['w'].values.flatten(), bins=wbins)\n",
    "# whist = scipy.stats.rv_histogram(whist_np)\n",
    "\n",
    "# np.save('u_test_hist.npy', uhist.pdf(ubins))\n",
    "# np.save('v_test_hist.npy', vhist.pdf(vbins))\n",
    "# np.save('w_test_hist.npy', whist.pdf(wbins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate 1D spectra, sample by sample\n",
    "# khub = 6\n",
    "# tmp1 = ds_test['up'].isel(time=0, z=khub)\n",
    "# tmp2 = np.fft.rfftn(tmp1.values, axes=(0,))\n",
    "# all_spectra = np.zeros((len(ds_test['time']), len(tmp2)))    \n",
    "    \n",
    "# for i in range(len(ds_test['time'])):\n",
    "#     up_xy_test = ds_test['up'].isel(time=i, z=khub)\n",
    "#     up_k_test = np.fft.rfftn(up_xy_test.values, axes=(0,))\n",
    "#     up_k_bar_test = np.mean(np.abs(up_k_test)**2, axis=1)\n",
    "    \n",
    "#     all_spectra[i,:] = up_k_bar_test.copy()\n",
    "# np.save('spectra_test.npy', all_spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate histograms that assess continuity\n",
    "## Helper variables\n",
    "# Simulation parameters\n",
    "dx, dy, dz = 15, 15, 15\n",
    "facx, facy, facz = 1/(4*dx), 1/(4*dy), 1/(4*dz)\n",
    "\n",
    "# Histogram parameters\n",
    "dmin = -0.02\n",
    "dmax = 0.02\n",
    "dbins = np.linspace(dmin, dmax, 500)\n",
    "all_dudx_hist = np.zeros((len(ds_test['time']), len(dbins)))\n",
    "all_dvdy_hist = np.zeros((len(ds_test['time']), len(dbins)))\n",
    "all_dwdz_hist = np.zeros((len(ds_test['time']), len(dbins)))\n",
    "all_div_hist = np.zeros((len(ds_test['time']), len(dbins)))\n",
    "\n",
    "## Iterate over timesteps\n",
    "for t in range(len(ds_test['time'])):\n",
    "    if t % 250==0: print(t, '...')\n",
    "    vel = ds_test.isel(time=t)\n",
    "\n",
    "    # Calculate gradients and divergence\n",
    "    dudx = facx*(-vel['u'].roll(x=-1,y=-1,z=-1,roll_coords=False).values + vel['u'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "                 -vel['u'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values + vel['u'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "                 -vel['u'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values + vel['u'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "                 -vel['u'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['u'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "    dvdy = facy*(-vel['v'].roll(x=-1,y=-1,z=-1,roll_coords=False).values - vel['v'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "                 +vel['v'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values + vel['v'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "                 -vel['v'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values - vel['v'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "                 +vel['v'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['v'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "    dwdz = facz*(-vel['w'].roll(x=-1,y=-1,z=-1,roll_coords=False).values - vel['w'].roll(x=0,y=-1,z=-1,roll_coords=False).values \\\n",
    "                 -vel['w'].roll(x=-1,y=0 ,z=-1,roll_coords=False).values - vel['w'].roll(x=0,y=0 ,z=-1,roll_coords=False).values \\\n",
    "                 +vel['w'].roll(x=-1,y=-1,z=0 ,roll_coords=False).values + vel['w'].roll(x=0,y=-1,z=0 ,roll_coords=False).values \\\n",
    "                 +vel['w'].roll(x=-1,y=0 ,z=0 ,roll_coords=False).values + vel['w'].roll(x=0,y=0 ,z=0 ,roll_coords=False).values)\n",
    "    div = dudx + dvdy + dwdz\n",
    "\n",
    "    # Calculate histograms\n",
    "    dudxhist_np = np.histogram(dudx.flatten(), bins=dbins)\n",
    "    dudxhist = scipy.stats.rv_histogram(dudxhist_np)\n",
    "\n",
    "    dvdyhist_np = np.histogram(dvdy.flatten(), bins=dbins)\n",
    "    dvdyhist = scipy.stats.rv_histogram(dvdyhist_np)\n",
    "\n",
    "    dwdzhist_np = np.histogram(dwdz.flatten(), bins=dbins)\n",
    "    dwdzhist = scipy.stats.rv_histogram(dwdzhist_np)\n",
    "\n",
    "    divhist_np = np.histogram(div.flatten(), bins=dbins)\n",
    "    divhist = scipy.stats.rv_histogram(divhist_np)\n",
    "\n",
    "    # Store histograms\n",
    "    all_dudx_hist[t,:] = dudxhist.pdf(dbins)\n",
    "    all_dvdy_hist[t,:] = dvdyhist.pdf(dbins)\n",
    "    all_dwdz_hist[t,:] = dwdzhist.pdf(dbins)\n",
    "    all_div_hist[t,:] = divhist.pdf(dbins)\n",
    "\n",
    "# Save histograms\n",
    "np.save('hist_dudx_test.npy', all_dudx_hist.mean(axis=0))\n",
    "np.save('hist_dvdy_test.npy', all_dvdy_hist.mean(axis=0))\n",
    "np.save('hist_dwdz_test.npy', all_dwdz_hist.mean(axis=0))\n",
    "np.save('hist_div_test.npy', all_div_hist.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daskenv202305",
   "language": "python",
   "name": "daskenv202305"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
